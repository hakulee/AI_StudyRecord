# :rocket:부스팅은 어떤 특징을 가진 앙상블 기법인가?
부스팅은 머신러닝 앙상블 기법 중 하나로 모델링을 만든 뒤 잘못된 부분을 다시 보정하여 다시 모델링을 계속하는 방법이다.  
>각 모델은 서로 영향을 주지않고 독립적으로 학습한다.

## :pencil2:부스팅 모델의 종류 및 특징
### 1. AdaBoost:  
잘못 분류된 샘플에 가중치(Weight)를 부여하여 다음 학습에 반영하는 알고리즘
- 장점:
  - 과적합 영항을 덜 받는다.
  - 구현이 쉽다.
  - 결정트리뿐만아니라 로지스틱 회귀, 선형회귀 등에도 사용할 수 있다.
- 단점:  
  - 이상치(outlier)에 민감하다.
  - 순차적으로 학습하기때문에 병렬처리가 어려워 연산량이 많다.

---

### 2. Gradient Boosting Machine (GBM):  
   이전 모델의 예측과 실제 값의 차이를 학습하여 다음 모델을 만드는 알고리즘
  - 장점:  
    - 뛰어난 예측 성능을 자랑하며, 다양한 손실 함수를 적용할 수 있어 회귀 및 분류 문제에 모두 유연하게 사용할 수 있다.  
    - 다양한 데이터 타입과 복잡한 관계를 잘 처리한다.
  - 단점:
    - 과적합(overfitting)에 취약할 수 있어 파라미터 설정 중요하다.
    - 병렬 처리가 어려워 학습 속도가 느리다.

---

### 3. XGBoost (eXtreme Gradient Boosting)
  XGBoost는 Gradient Boosting을 효율성, 유연성, 성능 측면에서 극대화한 알고리즘  
  (병렬 처리, 결측값 처리, 가지치기 등의 기법을 통해 빠른 학습속도와 높은 예측 정확도)
- 장점:
  - 과적합 방지 기능과 결측값 처리 기능이 내장되어 있어 모델의 안정성이 높다.
  - 뛰어난 성능과 빠른 속도로 Kaggle과 같은 데이터 분석 대회에서 자주 사용한다.
- 단점:
  - 메모리 사용량이 많다.
  - 많은 파라미터를 가지고 있어 복잡할 수 있다.

---

### 4. LightGBM (Light Gradient Boosting Machine)
   Leaf-wise 방식의 트리 균형을 맞추는 대신, 손실 감소가 가장 큰 리프 노드를 분할하는 알고리즘  
   (Gradient Boosting을 발전시킨 것이 XGBoost, 여기서 속도를 더 높인 것이 LightGBM)
- 장점:
  - 매우 빠른 학습 속도와 낮은 메모리를 사용한다.
  - 대규모 데이터에 효과적이다.
- 단점:
  - 데이터가 작을 때 과적합이 될 수 있다. (10,000개 이하의 데이터)
  <img width="431" height="485" alt="image" src="https://github.com/user-attachments/assets/f3f705fe-a346-4552-8c68-8c86caf1333c" />  
  (출처:https://mac-user-guide.tistory.com/79)

---

### 5. CatBoost (Categorical Boosting)
  CatBoost는 대부분이 범주형변수로 이루어진 데이터셋에서 예측 성능이 우수한 알고리즘
- 장점:
  - 별도의 전처리 과정이 필요 없어 편리하다.
  - 과적합 방지에 효과적인 기능들이 내장되어 있다.
- 단점:
  - XGBoost나 LightGBM에 비해 학습 속도가 느리다.
  - 다른 모델에 비해 수정할 파라미터가 적지만, 최적의 성능을 내기 위해선 파라미터의 많은 수정이 필요하다.

