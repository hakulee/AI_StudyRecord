# :gear:딥러닝 성능향상을 위한 주요 하이퍼파라미터(Hyperparameter)
하이퍼파라미터(Hyperparameter)는 모델학습 전 사람이 직접 설정해줘야 하는 값이다. 하이퍼파라미터 값의 설정은 모델의 성능에 상당한 영향을 미칠 수 있으며 최적의 하이퍼파라미터를 찾아 모델의 성능을 올리는 것이 중요하다. 다양한 하이퍼파라미터 조합은 그리드 서치(Grid search), 무작위 서치(Random Search) 또는 베이지안 최적화(Bayesian Optimization)와 같은 방법을 사용하여 모델의 성능을 평가할 수 있다.

## Learning Rate (학습률): 
- 모델이 가중치(Weight)를 업데이트하는 정도를 조절한다.
- 너무 크면 최적점을 지나칠 수 있고, 너무 작으면 학습 속도가 느려질 수 있다.
  (일반적으로 학습률은 0.0001에서 0.1 사이의 값을 사용)


## Epoch (에폭)
- 학습 데이터 전체를 의미
- 1 epoch은 전체 데이터셋을 한번 학습했다는 의미  
  (보통 여러번 학습함)


## Batch Size (배치 사이즈)
- 전체 데이터(epoch)를 한번에 학습하지 않고 여러 데이터셋으로 나눠 학습하는 샘플 수
- 메모리 및 모델 성능을 고려하여 에폭을 작은 데이터 셋으로 나누어 수행


## Optimizer (옵티마이저)
- 가중치(Weight)를 어떻게 업데이트할지 정하는 파라미터
- 최근에는 adam 옵티마이저를 가장 많이 사용


## 은닉층 수와 뉴런 수 (Network Architecture, 네트워크 아키텍처)
- Hidden Layers (은닉층 수)
    - 은닉층의 수가 증가할수록 모델은 더 복잡한 패턴을 학습
    - 하지만, 너무 많은 층은 학습을 어렵게 만들고 과적합의 위험 증가
    - 일반적으로 문제의 복잡성에 따라 2~5개의 은닉층으로 시작하여 점진적으로 조정

- Hidden Units (은닉층의 뉴런 수; 노드 수)
    - 각 층의 노드 수는 해당 층의 학습 능력을 결정
    - 노드 수가 많으면 더 복잡한 특성을 포착할 수 있지만, 과적합의 위험이 증가하고 계산 비용이 높아짐
    - 첫 Hidden Layer의 뉴런 수가 Input Layer 보다 큰 것이 효과적
      (보통 2의 거듭제돕 사용; 32, 64, 128...)
